= Data Mid-Bootcamp Project
:stylesheet: boot-darkly.css
:linkcss: boot-darkly.css
:image-url-ironhack: https://user-images.githubusercontent.com/23629340/40541063-a07a0a8a-601a-11e8-91b5-2f13e4e6b441.png
:my-name: Jorge Castro DAPT NOV2021
:description:
//:fn-xxx: Add the explanation foot note here bla bla
:url-dash-1: http://ec2-3-120-10-156.eu-central-1.compute.amazonaws.com:3000/d/tJQ_x4Enk/historic-air-quality-data-beijin-aotizhongxin-all-metrics?orgId=1&from=1356994800000&to=1483225200000
:url-dash-2: http://ec2-3-120-10-156.eu-central-1.compute.amazonaws.com:3000/d/DTwNf4Pnk/air-quality-data-feed-every-hour-past-24h-all-metrics-berlin-germany?orgId=1&from=1647164343360&to=1647250743360
:url-dataset: https://www.kaggle.com/sid321axn/beijing-multisite-airquality-data-set
:url-dataset2: https://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data
:url-api: https://www.weatherapi.com/docs/
:url-influx: https://www.influxdata.com
:url-grafana: https://grafana.com/
:url-insert: https://github.com/jecastrom/time_series_pro/blob/master/files_for_project/PRSA_Data_20130301-20170228/01_insert.ipynb
:url-query: https://github.com/jecastrom/time_series_pro/blob/master/files_for_project/PRSA_Data_20130301-20170228/02_query.ipynb
:url-live: https://github.com/jecastrom/time_series_pro/blob/master/files_for_project/PRSA_Data_20130301-20170228/03_live_data_weather_api_V2.ipynb
:toc:
:toc-title: Build and deploy a Data Ingestion Pipeline Pull
:toc-placement!:
:toclevels: 5
ifdef::env-github[]
:sectnums:
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
:experimental:
:table-caption!:
:example-caption!:
:figure-caption!:
:idprefix:
:idseparator: -
:linkattrs:
:fontawesome-ref: http://fortawesome.github.io/Font-Awesome
:icon-inline: {user-ref}/#inline-icons
:icon-attribute: {user-ref}/#size-rotate-and-flip
:video-ref: {user-ref}/#video
:checklist-ref: {user-ref}/#checklists
:list-marker: {user-ref}/#custom-markers
:list-number: {user-ref}/#numbering-styles
:imagesdir-ref: {user-ref}/#imagesdir
:image-attributes: {user-ref}/#put-images-in-their-place
:toc-ref: {user-ref}/#table-of-contents
:para-ref: {user-ref}/#paragraph
:literal-ref: {user-ref}/#literal-text-and-blocks
:admon-ref: {user-ref}/#admonition
:bold-ref: {user-ref}/#bold-and-italic
:quote-ref: {user-ref}/#quotation-marks-and-apostrophes
:sub-ref: {user-ref}/#subscript-and-superscript
:mono-ref: {user-ref}/#monospace
:css-ref: {user-ref}/#custom-styling-with-attributes
:pass-ref: {user-ref}/#passthrough-macros
endif::[]
ifndef::env-github[]
:imagesdir: ./
endif::[]

image::{image-url-ironhack}[width=70]

{my-name}


                                                     
====
''''
====
toc::[]

{description}




== Objective

Build a platform to handle time series data (metrics and events) from IoT devices, store them on a database and visualize it on a dashboard, creating a dataset from data pulled from an API.

Technologies to be used:

* AWS EC2 linux instance
* Python
* Docker containers
** {url-influx}[InfluxDB (Open source Non-relational Time series database)]
** {url-grafana}[Grafana (Open source Data visualization web tool)]

== Data Sources

For this project I will use weather data, specifically Multi-Site Air-Quality Data:

=== Data set for historical data
{url-dataset}[Beijing Multi-Site Air-Quality Data Set (CSV files)]

{url-dataset2}[Beijing Multi-Site Air-Quality Data Set from the Machine Learning Repository]

	
	
This hourly data set considers 6 main air pollutants and 6 relevant meteorological variables at multiple sites in Beijing.
	
Measurements in Ug/m3

What unit is Ug m3?   


The concentration of an air pollutant (eg. ozone) is given in micrograms (one-millionth of a gram) per cubic meter air or µg/m3

	
`*_PM2.5:_*` refers to atmospheric particulate matter (PM) that have a diameter of less than 2.5 micrometers, which is about 3% the diameter of a human hair.
	
`*_PM10:_*` particles that are 10 micrometres or less, and are also called fine particles.
	
`*_SO2:_*` Sulphur dioxide is a colourless gas with an irritating pungent odour. It readily dissolves in water and is one of the main chemicals that causes acid rain. Sulphur dioxide is a common air pollutant. ... One of the main uses of sulphur dioxide is as a chemical intermediate in the production of sulphuric acid.
	
`*_NO2:_*` Nitrogen dioxide, or NO2, is a gaseous air pollutant composed of nitrogen and oxygen and is one of a group of related gases called nitrogen oxides, or NOx. NO2 forms when fossil fuels such as coal, oil, gas or diesel are burned at high temperatures.
	
`*_CO:_*` Carbon monoxide is often referred to as ‘The Silent Killer’. This is because you can't smell, hear or see carbon monoxide, yet it is a highly toxic gas which can have devastating consequences.
	
Carbon Monoxide (chemical symbol: CO) is a colourless, odourless and tasteless gas created by the incomplete combustion of fossil fuels (gas, oil, coal and wood). It is highly toxic to humans and animals.
	
`*_O3:_*` Ozone at ground level is a harmful air pollutant, because of its effects on people and the environment, and it is the main ingredient in “smog."
	
_TEMP_: temperature (degree Celsius)
	
_PRES_: pressure (hPa): Atmospheric pressure, also known as barometric pressure (after the barometer), is the pressure within the atmosphere of Earth. The standard atmosphere (symbol: atm) is a unit of pressure defined as 101,325 Pa (1,013.25 hPa; 1,013.25 mbar), which is equivalent to 760 mm Hg, 29.9212 inches Hg, or 14.696 psi.
	
_DEWP_: dew point temperature (degree Celsius): The dew point is the temperature below which the water vapour in a volume of air at a constant pressure will condense into liquid water. It is the temperature at which the air is saturated with moisture.
	
	RAIN: precipitation (mm)
	
	wd: wind direction
	
	WSPM: wind speed (m/s)
	
	station: name of the air-quality monitoring site


image::https://user-images.githubusercontent.com/63274055/155881047-89febaf1-7c06-45c8-97c5-3fe312119177.png[width=800]

=== External API for real-time data

{url-api}[Weather API]

	
====
''''
====

== How the final product is going to look like

The first part is the database storing the data. For this I will be using InfluxDB which is a time series database and better suited to handle this type of data without too much overhead.

In InfluxDB we can build queries and visualize them there on an integrated graphical interface. However I will not be using InfluxDB to visualize the data as we cannot allow the stakeholders and users that will be consuming this data to have direct access to the database. 

image::https://user-images.githubusercontent.com/63274055/155841851-0e80a26f-431e-45a7-88f2-113ae0598fbf.gif[width=800]

It is best to have a front end that will display some sort of dashboard such as Grafana in our case.

image::https://user-images.githubusercontent.com/63274055/155845094-0c165537-ce41-4324-83d4-5d439743c28e.gif[width=800]

====
''''
====

== What I am going to build


The platform is going to have 2 parts:

* Part I: Ingestion of the CSV files
* Part II: Ingestion of the API json files



image::https://user-images.githubusercontent.com/63274055/155882508-9521a27a-ab84-4b6f-9b1e-ed2dd7f3ffd5.png[width=800]

* Part I:

. CSV files to be worked on which are the time series dataset

. Then I am going to create a Python ingestion program. This program is going to take the csv file,

. and is going to write it into the InfluxDB database, hosted as a Docker container. Once the data is in InfluxDB, (where the data and aggregations are stored) 

. then I can set up a dashboard in Grafana, also hosted in a Docker container and query the data from InfluxDB

. The "Client" is the pc of the stakeholders from which the Grafana UI will be accessed to the AWS EC2 instance with an URL

* Part II:

. Accesing the external Weather API

. and have a Python integration script which will query data from the Weather API and then write it to 

. the InfluxDB database 

.  Finlay the steps 4 and 5 repeat as in Part I.

== Choice between relational DB vs Non-Relational Time Series DB

When we deal with time series data, nothing stops us from using a relational database. This is the case specially when the amount of data is not significant. However if we are going to have a lot of traffic, sensor readings, this normally means massive quantities of data. In this case then a non-sql database and specifically a time series database is the best option.

Relational databases are not designed to have a timestamp as the primary key (PK), as an index. They need to be generic and allow for sorting and querying according to multiple different columns, keys and indexes, TSDBs are specific for querying and sorting data according to its timestamp (PK) and are therefore much more efficient and faster when doing that compared to relational databases.

One very useful functionallity of TSDB's to keep resources to an cost efficient level, they handle retention periods. Also calculations and aggregations are processed within the database making them faster.

== Schema design

image::https://user-images.githubusercontent.com/63274055/155876108-16779d53-ddd2-448f-8f19-950f51feda3e.png[width=800]

Now that we know how our data is structured we look at our access patters, how do we actually want to access our data.

What is access pattern in NoSQL? Access patterns or query patterns define how the users and the system access the data to satisfy business needs.  

So how do we want to access the data? In our case, the most important element is time, so by timestamp, so we are able to query data from a point A to point B in time.

Second element to query the data is `*_where from_*` we want the data, `*_where was this data measured_*`. For location we would query by the weather _station._ This two elements, the `*_timestamp_*` and the `*_station_*` are our main layer of access pattern.

The other access pattern, 3rd layer would be to query by an specific `*_sensor_*` such as wind speed

Access patterns:   


1.- Timestamp  

2.- Station  

3.- Sensor

image::https://user-images.githubusercontent.com/63274055/155858881-87e70763-f16b-4e37-bed6-dc420b490b99.png[width=400]

=== Schema design to be implemented

The TimeStamp will be in the format of Zulu time
A timestamp in Zulu format would look like TZ. That is a date “YYYY-MM-DD” with the four-digit Year, two-digit month and two-digit day, “T” for “time,” followed by a time formatted as “HH:MM:SS” with hours, minutes and seconds, all followed with a “Z” to denote that it is Zulu format.

Zulu (short for "Zulu time") is used in the military and in navigation generally as a term for Universal Coordinated Time (UCT), sometimes called Universal Time Coordinated ( UTC ) or Coordinated Universal Time (but abbreviated UTC), and formerly called Greenwich Mean Time.

image::https://user-images.githubusercontent.com/63274055/155892558-3f9526e5-092b-4685-aac7-e8d7fcb7f8b0.png[width=800]

== How is the data stored in the database (InfluxDB)

image::https://user-images.githubusercontent.com/63274055/155885913-00b3804f-9caf-4932-a586-13e4b244b72e.png[width=800]




== Setting up Python virtual environment 

* I provisioned a EC2 instance in AWS running Ubuntu. Docker is already installed by default on the EC2 Cloud9 instance.

image::https://user-images.githubusercontent.com/63274055/155895818-2b6befbe-50d1-400e-bbdc-e47f77fe380b.png[width=800]

====
''''
====

* On the security group, I have set two Inbound Rules (this are nothing but firewall rules). I have opened the port 8086 to access InfluxDB and the port 3000 to access Grafana using the `Public IPv4 DNS` name.

image::https://user-images.githubusercontent.com/63274055/155896199-3155b474-e69c-48a0-ac62-83b0969560de.png[width=600]

====
''''
====


* To avoid having a new IP address every time I stop and start the instance, in order to have a fixed Public IPv4 address, I set up Elastic IP and associated it to the EC2 Cloud9 instance.

image::https://user-images.githubusercontent.com/63274055/155896874-81e602ec-e11c-4827-8c65-c52646a5fb40.png[width=800]

====
''''
====


* Create a directory for the project and cd into it.  


** To create the Python virtual environment:

```bash
python3 -m venv project-venv
```
** To verify that the virtual environment was installed:

```bash
ls -ltr project-venv/bin
```

** To activate the virtual environment:

```bash
source project-venv/bin/activate
```

== Setting up the development environment (Docker containers for InfluxDB and Grafana)

* In order to download the docker images and setup the  parameters I need (usernames and passwords), I created a Docker compose file (yml file)

* Before running the command `docker-compose up`, inside the folder to be used for the project I had to create a folder named *gfdata*, and set the folder permissions to 777. Otherwise, the deployment of the docker container of Grafana will fail.

Having already uploaded the docker-compose.yml file, on the EC2 cloud9 terminal, from the folder we have created to have our dev environment, we run the following command:

```
docker-compose up
```


```yml
version: '2'
services:
  influxdb:
    image: influxdb:2.0
    ports:
      - '8086:8086' 
    volumes:
      - ./data:/var/lib/influxdb2
      - ./config:/etc/influxdb2
    environment:
      - DOCKER_INFLUXDB_INIT_MODE=setup
      - DOCKER_INFLUXDB_INIT_USERNAME=my-user
      - DOCKER_INFLUXDB_INIT_PASSWORD=my-password
      - DOCKER_INFLUXDB_INIT_ORG=my-org 
      - DOCKER_INFLUXDB_INIT_BUCKET=air-quality

  grafana:
    image: grafana/grafana:8.1.1
    ports:
      - '3000:3000'
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - INFLUXDB_DB=db0
      - INFLUXDB_ADMIN_USER=admin
      - INFLUXDB_ADMIN_PASSWORD=pw12345
      - INFLUXDB_ADMIN_USER_PASSWORD=admin
      - INFLUXDB_USERNAME=user
      - INFLUXDB_PASSWORD=user12345
    volumes:
      - ./gfdata:/var/lib/grafana

```



If for some reason, the instance stops, to start again all the docker containers, on the terminal run:

```
docker restart $(docker ps -a -q)
```


to verify that the containers are running:

```
docker ps
```

image::https://user-images.githubusercontent.com/63274055/155902967-f700133d-1d89-4eeb-a806-15be535441fa.png[width=800]

== Installing the necessary Python libraries on EC2 virtual environment

* On the EC2 terminal, within the Python virtual environment folder, I had to install the Python libraries needed to run the pipeline:

* InfluxDB Python client
* Requests
* Pandas
* Numpy
* Schedule

====
''''
====

== Setting up InfluxDB 

Created a token in InfluxDB so I was able to write data into the database from the python client.

.. Create a Bucket (already created when running the docker compose file)

.. Create a token:

Data => Tokens => Create tokens = All access token, added a description: Python Client

image::https://user-images.githubusercontent.com/63274055/155898519-a1ec354a-5f59-4dd0-bd4d-8dfaf9674a45.png[width=400]

Then we copy the token to be used in the connection string.

== Building Part I, steps 1, 2 and 3 (Inserting data to the database from csv dataset)

image::https://user-images.githubusercontent.com/63274055/156252197-e2730015-82fa-4401-98cf-81d4579e04e4.png[width=800]

{url-insert}[Building Part I, steps 1, 2 and 3 Jupyter Notebook]



```py
import pandas as pd
import datetime as dt
from numpy import float64

# Import the new influxdb API client
import influxdb_client
from influxdb_client.client.write_api import SYNCHRONOUS
```

```py
# Read the csv into a dataframe
df = pd.read_csv("PRSA_Data_Aotizhongxin_20130301-20170228.csv")
```
```py
# Dropping any row that has at least one null value
df.dropna(axis=0, how='any', inplace=True)
```


```py

# As there is no timestamp on the dataset, only 4 separate column
# I am creating a timestamp out of the four columns
# needed for influx 2020-01-01T00:00:00.00Z
# lambda s : dt.datetime(*s) takes every row and convert them -> *s
# strftime to reformat the string into influxdb timestamp format
df['TimeStamp'] = df[['year', 'month', 'day', 'hour']].apply(
    lambda s: dt.datetime(*s).strftime('%Y-%m-%dT%H:%M:%SZ'), axis=1)
```


```py
# setting the timestamp as the index of the dataframe
df.set_index('TimeStamp', inplace=True)
```

```py
# dropping the year, month, day, hour, No from the dataframe
converted_ts = df.drop(['year', 'month', 'day', 'hour', 'No'], axis=1)
print(converted_ts)
```

```py
# Changing the column types to float
ex_df = converted_ts.astype({"PM2.5": float64,
                             "PM10": float64,
                             "SO2": float64,
                             "NO2": float64,
                             "CO": float64,
                             "O3": float64,
                             "TEMP": float64,
                             "PRES": float64,
                             "DEWP": float64,
                             "RAIN": float64,
                             "WSPM": float64})


```

```py
# Defining tag fields
datatags = ['station', 'wd']
```

```py
#Setting up Database
client = influxdb_client.InfluxDBClient(
    url='ec2-35-157-40-78.eu-central-1.compute.amazonaws.com:8086',
    token='',
    org='my-org'
)
```

```py
# Writing the data with two tags
write_api = client.write_api(write_options=SYNCHRONOUS)
message = write_api.write(bucket='air-quality', org='my-org', record=ex_df,
data_frame_measurement_name='full-tags', data_frame_tag_columns=['station', 'wd'])
print(message)
```

```py
# The asynchronous buffering API to Write time-series data into InfluxDB.
# This API always buffers points/lines to create batches under the hood to optimize data 
# transfer to InfluxDB server, flush is used to send the buffered data to InfluxDB immediately.
# I use flush() after each write as I was getting a "time out error message"
write_api.flush()
```



```py
# Writing the data only with one tag
write_api = client.write_api(write_options=SYNCHRONOUS)
message = write_api.write(bucket='air-quality', org='my-org', record=ex_df,
data_frame_measurement_name='location-tag-only', data_frame_tag_columns=['station'])
print(message)
```

```py
write_api.flush()
```


== Querying the data with Python

{url-query}[Querying the air quality data from InfluxDB using Python. Jupyter Notebook]


Now that I have successfully written the data to InfluxDB, instead of querying the data with the InfluxDB UI, I will do it using Python. In this way, an end user has the flexibility to work with the data using a Pandas DataFrame.


* Importing libraries

```py
import pandas as pd
import influxdb_client
from influxdb_client.client.write_api import SYNCHRONOUS
import influxdb_client.client.influxdb_client
```

* Created a the connection string to InfluxDB

```py
#Setting up Database (Connection string)
client = influxdb_client.InfluxDBClient(
    url='ec2-35-157-40-78.eu-central-1.compute.amazonaws.com:8086',
    token='z7o_5pRpzKhz6yv9eJezq575NsOfQJkzVukK8YfSkrqZ_B-RHZs1f2mYoOR67J6mKb_Wh_clzBuhz0AVTTW0Sg==',
    org='my-org'
)
```



* Instantiate the query client

```py
queryAPI = client.query_api()
```

[NOTE]
====
Unpacking this sentence:

In programming, `instantiation` is the creation of a real `instance` or particular realization of an abstraction or template such as a class of objects or a computer process. To instantiate is to create such an instance by, for example, defining one particular variation of object within a class, giving it a name, and locating it in some physical place.

An instance, in object-oriented programming (OOP), is a specific realization of any object.
====


* Created a flux query and store it on a variable (Flux is the InfluxDB query language)


```py
myquery_location = '''
from(bucket: "air-quality") |> range(start: 2013-03-25T00:00:00Z, stop: 2013-05-01T00:00:00Z)' \
'|> filter(fn: (r) => r["_measurement"] == "location-tag-only")' \
'|> filter(fn: (r) => r["_field"] == "TEMP")
'''
```

* Creating a DataFrame with the data I am querying
```py
location_df = queryAPI.query_data_frame( query= myquery_location)
```

* To see the data

```py
print(location_df.info())
print(location_df)
```

* A second query

```py
myquery_everything = '''
from(bucket: "air-quality") |> range(start: 2013-03-25T00:00:00Z, stop: 2013-05-01T00:00:00Z)' \
'|> filter(fn: (r) => r["_measurement"] == "full-tags")' \
'|> filter(fn: (r) => r["_field"] == "TEMP")'''
```

```py
everything_df = queryAPI.query_data_frame( query= myquery_everything)
```
```py
print(everything_df)
```


TIP: I have successfully been able to query the air quality data in InfluxDB running on a AWS EC2 Docker container from my local Jupyter notebook

image::https://user-images.githubusercontent.com/63274055/155985152-0100b519-e6db-481d-88c1-dc35b3487186.png[width=800]


== Building Part I, step 4 and 5 (Setting up a Grafana dashboard in the AWS EC2 instance and query the data)

image::https://user-images.githubusercontent.com/63274055/156251632-20b23e64-4707-4d09-96c6-ce6e06971a81.png[width=800]

1.- Connecting Grafana to our database InfluxDB


* In the Grafana UI go  to: `Configuration => Data sources => Add data source`

image::https://user-images.githubusercontent.com/63274055/156043501-e161f486-5a99-4833-a517-b260ba62f8c9.png[width=600]

* Complete the data source information:

image::https://user-images.githubusercontent.com/63274055/156050397-2c12786c-21e5-4ffc-adbc-208e8350ed0e.png[width=800]

====
''''
====

2.- Grafana dashboard for InfluxDB

The most important element in Grafana to setup a dashboard is adding the InfluxDB query, which has to be in Flux scripting language.

While I improve my Flux fluency, which is as good as my Latin, I grabbed the query from the query builder of InfluxDB which is pretty handy.

image::https://user-images.githubusercontent.com/63274055/156060667-003edbd6-1af8-4a38-aa5c-33824b8e0b0c.png[width=800]

====
''''
====

Here is the Grafana dashboard with Historical Air Quality data from Shenyang-Aotizhongxin, China.

image::https://user-images.githubusercontent.com/63274055/156063961-85025498-76b5-43da-9269-c4ef68418c91.png[width=800]


== Building Part II, step 1 and 2 (Ingestion of live Data from the Weather API to InfluxDB on AWS EC2 instance)

[NOTE]
====
What is an API:

APIs, short for Application Programming Interfaces, are software-to-software interfaces. Meaning, they allow different applications to talk to each other and exchange information or functionality. This allows businesses to access another business’s data, piece of code, software, or services in order to extend the functionality of their own products — all while saving time and money.  


====

[NOTE]
====
What is an API Call:

An API call is the process of a client application submitting a request to an API and that API retrieving the requested data from the external server or program and delivering it back to the client.
====


image::https://user-images.githubusercontent.com/63274055/156236321-2d170d60-0229-4453-b937-2b251ecc688c.png[width=800]

{url-live}[Ingestion of live Data from the Weather API to InfluxDB on AWS EC2 instance Jupyter Notebook]

====
''''
====

In order to separate the historical data coming from the csv files and the live data coming from the weather API, I needed to create a different bucket in InfluxDB.

image::https://user-images.githubusercontent.com/63274055/156357096-7b137d2f-7c5c-4a9f-80ee-5a1b48e1b901.png[width=400]

====
''''
====

This are the steps we go through in this script:

* To query the API with the "request" library as specified in the  https://www.weatherapi.com/docs/[Weather API documentation]  
* Format the timestamp
* "Normalize" or flatten in tabular form the json object with the json_normalize() function
* Rename the columns
* Filter out the columns we need on a DataFrame
* Set up the connection string and call the API using the InfluxDB client as outlined in the  https://docs.influxdata.com/influxdb/cloud/api-guide/client-libraries/python/[InfluxDB documentation]


* Write the data into InfluxDB https://www.influxdata.com/blog/writing-data-to-influxdb-with-python/[(influxDB documentation 1)] and https://docs.influxdata.com/influxdb/cloud/api-guide/client-libraries/python/[(influxDB documentation 2)]

====
''''
====

* Import libraries
```py
from numpy import float64, int32, string_
import requests
import json
import pandas as pd
from pandas import json_normalize
import datetime as dt
```

* Import the influxdb API client

```py

import influxdb_client
from influxdb_client.client.write_api import SYNCHRONOUS
```
====
''''
====

* Load the configuration from the json file (Having the API key on a separate file, we are not exposing the key on the code publicly)
```py

with open("api_config.json") as json_data_file:
    config = json.load(json_data_file)
```
====
''''
====

* The Payload of an API Module is the body of your request and response message. It contains the data that you send to the server when you make an API request. You can send and receive Payload in different formats, for instance JSON.

* Here we can send the variable "Key" which contains the API key being read from the json file. The "q" is a query parameter of the API based on the location.

* "aqi" is a parameter to enable or disable to receive air quality data in forecast API output.
In the API documentation we obtain and compose the url to request the "current json" using the "request" library.
```py
payload = {'Key': config['Key'], 'q': 'Berlin', 'aqi': 'no'}
r = requests.get("http://api.weatherapi.com/v1/current.json", params=payload)
```
====
''''
====

* Here we create an object called r_string with the request and apply the .json() function to create the json file. If the response is not written in json format, it would give us an error message.
```py
r_string = r.json()
print(r_string)`
```
image::https://user-images.githubusercontent.com/63274055/156451415-c28005d0-ae25-4a3c-a2c7-1265c6cc16d9.png[width=800]

* In the following image, we can have a better view of the nested json response:

image::https://user-images.githubusercontent.com/63274055/156452722-88ffeb49-5dd7-407f-aa6c-61b21f8d2424.png[width=300]

image::https://user-images.githubusercontent.com/63274055/156452846-d252afc4-583c-4fe1-9051-403510def163.png[width=400]
====
''''
====
* In the previous cell, we can see the response, we obtain a json file with nested fields. So for us to deal effectively with this data, it is best to deal with a table layout, where we can have column names. So we normalize the nested json with the json_normalize() function. To "normalize" in this context is to convert the nested json or this  semi-structured JSON data into tabular data, a flat table.
```py
normalized = json_normalize(r_string)
```
image::https://user-images.githubusercontent.com/63274055/156461304-8a69c7be-cfa0-4748-b7cd-33eb9fcdbd60.png[width=600]

image::https://user-images.githubusercontent.com/63274055/156462078-8db4fee1-e99f-491c-8623-a26a796bf379.png[width=600]
====
''''
====
* Adding a new column 'TimeStamp'.
* Transforming the time from "localtime_epoch" to YYYY-MM-DDTHH:MM:SS format.
* From Weather API we only get time in "local" time format, not in UTC time format. This is the reason why we have to have this timestamp format with +02.00 which is very important (for Berlin) otherwise TS will be in UTC and therefore in the future -> it will not get shown on the board
```py
normalized['TimeStamp'] = normalized['location.localtime_epoch'].apply(lambda s : dt.datetime.fromtimestamp(s).strftime('%Y-%m-%dT%H:%M:%S+02:00'))
```
====
''''
====

* After the json "normalization" we have to rename the columns with names meaningful to us.
```py
normalized.rename(columns={'location.name': 'location', 
      'location.region': 'region',
      'current.temp_c': 'temp_c',
      'current.wind_kph': 'wind_kph'
      }, inplace=True)     
print(normalized)
print(normalized.dtypes)
```
====
''''
====

* Here again, we must set the timestamp as the index as its the norm for our time series database 
```py
normalized.set_index('TimeStamp', inplace = True)
```
====
''''
====

* The ex_df is the final DataFrame to export or to write into the database. Also we filter out just the columns that we are going to export, in our case, temp and wind for export. So we select only the data we need from the whole bunch of data that we fetched from the API response.

```py
ex_df = normalized.filter(['temp_c','wind_kph'])      

print(ex_df)
print(ex_df.dtypes)
```
====
''''
====

* Setting up Database (Connection string) using the InfluxDB client
```py
client = influxdb_client.InfluxDBClient(
    url='ec2-35-157-40-78.eu-central-1.compute.amazonaws.com:8086',
    token='z7o_5pRpzKhz6yv9eJezq575NsOfQJkzVukK8YfSkrqZ_B-RHZs1f2mYoOR67J6mKb_Wh_clzBuhz0AVTTW0Sg==',
    org='my-org'
)
```
====
''''
====

* Write the data to the database InfluxDB into measurement
```py
write_api = client.write_api(write_options=SYNCHRONOUS)
message = write_api.write(bucket='live_weather',org='my-org',record = ex_df, data_frame_measurement_name = 'api')
write_api.flush()
print(message)
```


== Automating workflow: Data Ingestion from the Weather API to InfluxDB on AWS EC2 instance
https://github.com/jecastrom/time_series_pro/blob/master/files_for_project/PRSA_Data_20130301-20170228/04_live_data_weather_api_schedule.ipynb[Automating workflow: Jupyter Notebook]


image::https://user-images.githubusercontent.com/63274055/156502444-0cb04a4b-65a5-46ec-9d45-fcf1bbb2ab81.png[width=800]

There are many ways we can automate this tasks:  

* CRON job

* AWS Lambda 

* AirFlow

* Python libraries

Considering that we only have two source systems and one target system, I used the Python Schedule library

```py
import time
import schedule
from numpy import float64, int32, string_
import requests
import json
import pandas as pd
from pandas import json_normalize
import datetime as dt
```

```py
def run_script():
    with open('live_data_weather_api.py') as f:
        script = f.read()
    exec(script)
    print('Script executed')
schedule.every(1).hours.do(run_script)

while True:
    schedule.run_pending()
    time.sleep(1)
```

I have created two executable python files and are running on the EC2 instance:

image::https://user-images.githubusercontent.com/63274055/156560998-7fa898b5-4c22-401b-99cc-93d1c01ba848.png[width=800]

* `*_live_data_weather_api.py_*`: Script for the data ingestion from the weather API

* `*_scheduler_*`: Job Scheduler to run the script every hour.

To verify what Python processes are running:

```bash
ps -aux | grep "python"
```



TIP: It works! The job scheduler is working as expected, reading the data weather from the API, processing it and writing the DataFrame to InfluxDB at the specified interval.



image::https://user-images.githubusercontent.com/63274055/156603499-98bab7b2-db1f-4be2-a126-dfee7656f39f.png[width=400]

====
''''
====

image::https://user-images.githubusercontent.com/63274055/156604024-c2858d12-2aa8-48a8-adfa-ec3557a0dc28.png[width=400]

====
''''
====

== The final Data Pipeline (Animated schema)

https://youtu.be/VVKKyVp-L88[Animation of the Data Pipeline (Video)]

== Direct link to the Grafana dashboards hosted on the EC2 instance

{url-dash-1}[Historic Air Quality Data from Beijin-Aotizhongxin, China - All metrics]  

{url-dash-2}[Air Quality Data Live Feed - All Metrics-  Berlin - Germany]


xref:Data-Mid-Bootcamp-Project[Top Section]




////

xref:Data Mid-Bootcamp Project[Top Section]

xref:Last-section[Bottom section]

//bla bla blafootnote:[{fn-xxx}]


//`*_Answer:_*`


.Unordered list title
* gagagagagaga
** gagagatrtrtrzezeze
*** zreu fhjdf hdrfj 
*** hfbvbbvtrtrttrhc
* rtez uezrue rjek  

.Ordered list title
. rwieuzr skjdhf
.. weurthg kjhfdsk skhjdgf
. djhfgsk skjdhfgs 
.. lksjhfgkls ljdfhgkd
... kjhfks sldfkjsdlk


sdsdsd

[,sql]
----
----



[NOTE]
====
A sample note admonition.
====
 
TIP: It works!
 
IMPORTANT: Asciidoctor is awesome, don't forget!
 
CAUTION: Don't forget to add the `...-caption` document attributes in the header of the document on GitHub.
 
WARNING: You have no reason not to use Asciidoctor.

bla bla bla the 1NF or first normal form.footnote:[{1nf}]Then wen bla bla


====
- [*] checked
- [x] also checked
- [ ] not checked
-     normal list item
====
[horizontal]
CPU:: The brain of the computer.
Hard drive:: Permanent storage for operating system and/or user files.
RAM:: Temporarily stores information the CPU uses during operation.






bold *constrained* & **un**constrained

italic _constrained_ & __un__constrained

bold italic *_constrained_* & **__un__**constrained

monospace `constrained` & ``un``constrained

monospace bold `*constrained*` & ``**un**``constrained

monospace italic `_constrained_` & ``__un__``constrained

monospace bold italic `*_constrained_*` & ``**__un__**``constrained

////
