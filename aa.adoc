= Data Mid-Bootcamp Project
:stylesheet: boot-darkly.css
:linkcss: boot-darkly.css
:image-url-ironhack: https://user-images.githubusercontent.com/23629340/40541063-a07a0a8a-601a-11e8-91b5-2f13e4e6b441.png
:my-name: Jorge Castro DAPT NOV2021
:description:
//:fn-xxx: Add the explanation foot note here bla bla
:url-dataset: https://www.kaggle.com/sid321axn/beijing-multisite-airquality-data-set
:url-dataset2: https://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data
:url-api: https://www.weatherapi.com/docs/
:url-influx: https://www.influxdata.com
:url-grafana: https://grafana.com/
:toc:
:toc-title: 
:toc-placement!:
:toclevels: 5
ifdef::env-github[]
:sectnums:
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
:experimental:
:table-caption!:
:example-caption!:
:figure-caption!:
:idprefix:
:idseparator: -
:linkattrs:
:fontawesome-ref: http://fortawesome.github.io/Font-Awesome
:icon-inline: {user-ref}/#inline-icons
:icon-attribute: {user-ref}/#size-rotate-and-flip
:video-ref: {user-ref}/#video
:checklist-ref: {user-ref}/#checklists
:list-marker: {user-ref}/#custom-markers
:list-number: {user-ref}/#numbering-styles
:imagesdir-ref: {user-ref}/#imagesdir
:image-attributes: {user-ref}/#put-images-in-their-place
:toc-ref: {user-ref}/#table-of-contents
:para-ref: {user-ref}/#paragraph
:literal-ref: {user-ref}/#literal-text-and-blocks
:admon-ref: {user-ref}/#admonition
:bold-ref: {user-ref}/#bold-and-italic
:quote-ref: {user-ref}/#quotation-marks-and-apostrophes
:sub-ref: {user-ref}/#subscript-and-superscript
:mono-ref: {user-ref}/#monospace
:css-ref: {user-ref}/#custom-styling-with-attributes
:pass-ref: {user-ref}/#passthrough-macros
endif::[]
ifndef::env-github[]
:imagesdir: ./
endif::[]

image::{image-url-ironhack}[width=70]

{my-name}


                                                     
====
''''
====
toc::[]

{description}


== Build and deploy a Data Ingestion Pipeline Pull

== Objective

Build a platform to handle time series data (metrics and events) from IoT devices, store them on a database and visualize it on a dashboard.



== Data Sources

For this project I will use weather data, specifically Multi-Site Air-Quality Data:

=== Data set for historical data
{url-dataset}[Beijing Multi-Site Air-Quality Data Set (CSV files)]

{url-dataset2}[Beijing Multi-Site Air-Quality Data Set from the Machine Learning Repository]

=== External API for real-time data

{url-api}[Weather API]


====
''''
====
Technologies to be used:

* AWS EC2 linux instance
* Python
* Docker containers
** {url-influx}[InfluxDB (Open source Time series database)]
** {url-grafana}[Grafana (Open source Data visualization web tool)]


====
''''
====

== How the final product is going to look like

The first part is the database storing the data. For this I will be using InfluxDB which is a time series database and better suited to handle this type of data without too much overhead.

In InfluxDB we can build queries and visualize them there on an integrated graphical interface. However I will not be using InfluxDB to visualize the data as we cannot allow the stakeholders that will be consuming this data to have direct access to the database. 

image::https://user-images.githubusercontent.com/63274055/155841851-0e80a26f-431e-45a7-88f2-113ae0598fbf.gif[width=800]

The to visualize..   

image::https://user-images.githubusercontent.com/63274055/155845094-0c165537-ce41-4324-83d4-5d439743c28e.gif[width=800]
====
''''
====

== What I am going to build


The platform is going to have 2 parts:

* Part I: Ingestion of the CSV files
* Part II: Ingestion of the API json files



image::https://user-images.githubusercontent.com/63274055/155846581-3e8dd67a-1943-432e-9011-ab60ba348538.png[width=900]

* Part I:

. CSV files to be worked on which are the time series dataset

. Then I am going to create a Python ingestion program. This program is going to take the csv file,

. and is going to write it into the InfluxDB database, hosted as a Docker container. Once the data is in InfluxDB, (where the data and aggregations are stored) 

. then I can set up a dashboard in Grafana, also hosted in a Docker container and query the data from InfluxDB

. The "Client" is the pc of the stakeholders from which the Grafana UI will be accessed to the AWS EC2 instance with an URL

* Part II:

. Accesing the external Weather API

. and have a Python integration script which will query data from the Weather API and then write it to 

. the InfluxDB database 

.  Finlay the steps 4 and 5 repeat as in Part I.


== Setting up the Docker containers for InfluxDB and Grafana

In order to download the docker images along with the parameters I need, I created a Docker compose file (yml file)

```yml
ersion: '2'
services:
  influxdb:
    image: influxdb:2.0
    ports:
      - '8086:8086' 
    volumes:
      - ./data:/var/lib/influxdb2
      - ./config:/etc/influxdb2
    environment:
      - DOCKER_INFLUXDB_INIT_MODE=setup
      - DOCKER_INFLUXDB_INIT_USERNAME=my-user
      - DOCKER_INFLUXDB_INIT_PASSWORD=my-password
      - DOCKER_INFLUXDB_INIT_ORG=my-org 
      - DOCKER_INFLUXDB_INIT_BUCKET=air-quality

  grafana:
    image: grafana/grafana:8.1.1
    ports:
      - '3000:3000'
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - INFLUXDB_DB=db0
      - INFLUXDB_ADMIN_USER=admin
      - INFLUXDB_ADMIN_PASSWORD=pw12345
      - INFLUXDB_ADMIN_USER_PASSWORD=admin
      - INFLUXDB_USERNAME=user
      - INFLUXDB_PASSWORD=user12345
    volumes:
      - ./gfdata:/var/lib/grafana

```

== Building Part I, steps 1, 2 and 3 (Inserting data to the database from csv dataset)

```python
import pandas as pd
from numpy import float64
import datetime as dt

# Import the new influxdb API client
import influxdb_client
from influxdb_client.client.write_api import SYNCHRONOUS

df = pd.read_csv("data/PRSA_Data_Aotizhongxin_20130301-20170228.csv", )

# As there is no timestamp on the dataset, only 4 separate column
# I am creating a timestamp out of the four columns
# needed for influx 2020-01-01T00:00:00.00Z
# lambda s : dt.datetime(*s) takes every row and convert them -> *s
# strftime to reformat the string into influxdb timestamp format
df['TimeStamp'] = df[['year', 'month', 'day', 'hour']].apply(
    lambda s: dt.datetime(*s).strftime('%Y-%m-%dT%H:%M:%SZ'), axis=1)

# setting the timestamp as the index of the dataframe
df.set_index('TimeStamp', inplace=True)
# dropping the year, month, day, hour, No from the dataframe
converted_ts = df.drop(['year', 'month', 'day', 'hour', 'No'], axis=1)
print(converted_ts)

# Changing the column types to float
ex_df = converted_ts.astype({"PM2.5": float64,
                             "PM10": float64,
                             "SO2": float64,
                             "NO2": float64,
                             "CO": float64,
                             "O3": float64,
                             "TEMP": float64,
                             "PRES": float64,
                             "DEWP": float64,
                             "RAIN": float64,
                             "WSPM": float64})


# Defining tag fields
datatags = ['station', 'wd']


client = influxdb_client.InfluxDBClient(
    url='http://localhost:8086',
    token='L4-rOr-5BNyYrpHYsgUmGmRUMl1wFfyOjBmFpBjgl9tSjUOn0hFsfEfdDVx1YfDiaebEzWJtif8PvwT24-QcNg==',
    org='my-org'
)

# Writing the data with two tags
write_api = client.write_api(write_options=SYNCHRONOUS)
message = write_api.write(bucket='air-quality', org='my-org', record=ex_df,
                          data_frame_measurement_name='full-tags', data_frame_tag_columns=['station', 'wd'])
print(message)

write_api.flush()

# Writing the data only with one tag
write_api = client.write_api(write_options=SYNCHRONOUS)
message = write_api.write(bucket='air-quality', org='my-org', record=ex_df,
                          data_frame_measurement_name='location-tag-only', data_frame_tag_columns=['station'])
print(message)

write_api.flush()
```

.


xref:Lab-xxxx[Top Section]

xref:Last-section[Bottom section]

//bla bla blafootnote:[{fn-xxx}]
//`*_Answer:_*`

////
.Unordered list title
* gagagagagaga
** gagagatrtrtrzezeze
*** zreu fhjdf hdrfj 
*** hfbvbbvtrtrttrhc
* rtez uezrue rjek  

.Ordered list title
. rwieuzr skjdhf
.. weurthg kjhfdsk skhjdgf
. djhfgsk skjdhfgs 
.. lksjhfgkls ljdfhgkd
... kjhfks sldfkjsdlk


sdsdsd

[,sql]
----
----



[NOTE]
====
A sample note admonition.
====
 
TIP: It works!
 
IMPORTANT: Asciidoctor is awesome, don't forget!
 
CAUTION: Don't forget to add the `...-caption` document attributes in the header of the document on GitHub.
 
WARNING: You have no reason not to use Asciidoctor.

bla bla bla the 1NF or first normal form.footnote:[{1nf}]Then wen bla bla


====
- [*] checked
- [x] also checked
- [ ] not checked
-     normal list item
====
[horizontal]
CPU:: The brain of the computer.
Hard drive:: Permanent storage for operating system and/or user files.
RAM:: Temporarily stores information the CPU uses during operation.






bold *constrained* & **un**constrained

italic _constrained_ & __un__constrained

bold italic *_constrained_* & **__un__**constrained

monospace `constrained` & ``un``constrained

monospace bold `*constrained*` & ``**un**``constrained

monospace italic `_constrained_` & ``__un__``constrained

monospace bold italic `*_constrained_*` & ``**__un__**``constrained

////
